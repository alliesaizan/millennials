{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pudding article collection, cleaning, and restructuring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from eventregistry import *\n",
    "import spacy\n",
    "import requests\n",
    "import bs4\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import pickle\n",
    "from itertools import chain\n",
    "import json\n",
    "\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "os.chdir(\"/Users/alliesaizan/Documents/Python-Tinkering/Pudding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentence_objects(tagged):\n",
    "    \"\"\"\n",
    "    This function finds the direct objects in the article title.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        objs = [i.text for i in tagged.noun_chunks if bool(re.search(\"dobj\", i.root.dep_)) == True]\n",
    "    except:\n",
    "        objs = \"\"\n",
    "    return(objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findall(sub, lst, overlap = True):\n",
    "    \"\"\"\n",
    "    This function finds the indicies where a sub-list occurs in a larger list.\n",
    "    I adapted this function from:\n",
    "    http://paddy3118.blogspot.com/2014/06/indexing-sublist-of-list-way-you-index.html\n",
    "    \"\"\"\n",
    "    sublen = len(sub)\n",
    "    firstthing = sub[0] if sub else []\n",
    "    indices, indx = [], -1\n",
    "    while True:\n",
    "        try:\n",
    "            indx = lst.index(firstthing, indx + 1)\n",
    "        except ValueError:\n",
    "            break\n",
    "        if sub == lst[indx : indx + sublen]:\n",
    "            indices.append(indx)\n",
    "            if not overlap:\n",
    "                indx += sublen - 1\n",
    "    return(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"(AUX\\s)*(ADV\\s)*(PART\\s)*(VERB\\s)+(ADV\\s)*(PART\\s)*\"\n",
    "\n",
    "def find_verb_phrases(doc):\n",
    "    \"\"\"\n",
    "    This function is designed to pull verb phrases from sentences where \n",
    "    millennials are the subject of the sentence. It assumes that the first \n",
    "    verb or verb phrase will refer to actions taken by millennials.\n",
    "    \"\"\"\n",
    "    # Obtain the parts of speech tags for each word in the title\n",
    "    pos_tags = \" \".join([i.pos_ for i in doc])\n",
    "    \n",
    "    # If the verb phrase pattern matches anywhere in the part of speech tags:\n",
    "    if re.search(pattern, pos_tags): \n",
    "        # Find the matching tags and extract them as a list of tags.\n",
    "        compiled = re.search(pattern, pos_tags)\n",
    "        compare_this = compiled.group().split()\n",
    "        # Compare this list of tags against the full list of tags for all the words in the title.\n",
    "        # Extract the indicies where the tags occur in the list.\n",
    "        result = findall(compare_this, pos_tags.split())[0]\n",
    "        # In the title, pull out the words with matching indicies.\n",
    "        verbs = \" \".join([i.text for i in doc][result:result + len(compare_this)])\n",
    "    else:\n",
    "        # If the title does not contain any verb phrases, just extract the first verb in the sentence\n",
    "        verbs = [i.text for i in doc if i.pos_ == \"VERB\"]\n",
    "        if len(verbs) != 0:\n",
    "            verbs = verbs[0]\n",
    "        else:\n",
    "            # If the sentence does not contain any words tagged as verbs, return an empty string\n",
    "            verbs = \"\"\n",
    "    return(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_verbs(doc):\n",
    "    \"\"\"\n",
    "    This function is designed to pull verbs from sentences. It extracts the\n",
    "    first verb because we only care about sentences where\n",
    "    millennials are the subject of the sentence.\n",
    "    \"\"\"\n",
    "    verbs = [i.lemma_ for i in doc if i.pos_ == \"VERB\"]\n",
    "    if len(verbs) != 0:\n",
    "        verbs = verbs[0]\n",
    "    else:\n",
    "        # If the sentence does not contain any words tagged as verbs, return an empty string\n",
    "        verbs = \"\"\n",
    "    return(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_valences(l, analyzer):\n",
    "    valences = [analyzer.polarity_scores(x)[\"compound\"] for x in l]\n",
    "    return pd.np.mean(valences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"b3b5aa5d-a173-4102-97e6-227c795f7349\"\n",
    "\n",
    "#er = EventRegistry(apiKey = api_key)\n",
    "\n",
    "#q = QueryArticlesIter(\n",
    "#    keywords = QueryItems.OR([\"millennials\", \"Millennials\", \"millenial\", \"Millenial\"]),\n",
    "#    lang = \"eng\",\n",
    "#    keywordsLoc=\"title\",\n",
    "#    dateStart = datetime.date(2015, 6, 16),\n",
    "#    dateEnd = datetime.date(2015, 10, 11),\n",
    "##    startSourceRankPercentile = 0,\n",
    "##    endSourceRankPercentile = 20,\n",
    "#    dataType = [\"news\"])\n",
    "\n",
    "#articles = pd.DataFrame(columns = [\"title\", \"url\", \"text\", \"date\"])\n",
    "\n",
    "#for art in q.execQuery(er, sortBy = \"date\"):    \n",
    "#    articles = articles.append({ \"title\": art[\"title\"], \"url\": art[\"url\"], \"text\": art[\"body\"], \"date\": art[\"date\"]}, ignore_index = True)\n",
    "\n",
    "# Save the articles dataset\n",
    "#pickle.dump(articles, open(\"articles.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial data frame cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pickle.load(open(\"articles.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format article titles\n",
    "articles[\"title\"] = articles[\"title\"].replace(\"&#\\d+|\\(|\\)\", \"\", regex = True)\n",
    "articles[\"title\"] = articles.title.apply(lambda x: re.split(\"\\s*(\\||;|\\.|\\s-\\s)\", str(x))[0])  # split on \"|\", \";\",\".\"\n",
    "articles[\"title_lower\"] = articles[\"title\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data and remove duplicates\n",
    "articles = articles[[\"title\", \"url\", \"text\", \"date\", \"title_lower\", \"tagged\"]]\n",
    "articles.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag the article titles and extract sentence verbs, subjects, and objects\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "articles[\"tagged\"] = articles[\"title\"].apply(nlp)\n",
    "\n",
    "articles[\"verbs\"] = articles[\"tagged\"].apply(find_verbs)\n",
    "\n",
    "articles[\"objects\"] = articles[\"tagged\"].apply(find_sentence_objects)\n",
    "articles[\"objects\"] = articles[\"objects\"].replace(\"^\\s+\", \"\", regex= True)\n",
    "\n",
    "articles[\"subject\"] = articles[\"tagged\"].apply(lambda x: [token.text.lower() for token in x if token.dep_ in [\"nsubj\", \"ROOT\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset on articles where millennials are the subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[\"mil_subj\"] = articles[\"subject\"].apply(lambda x: 1 if \"millennials\" in str(x).lower() else 0)\n",
    "millennial_articles = articles.loc[articles[\"mil_subj\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "millennial_articles[\"split_text\"] = millennial_articles[\"text\"].apply(sent_tokenize)\n",
    "millennial_articles[\"num_sentences\"] = millennial_articles[\"split_text\"].apply(len)\n",
    "millennial_articles = millennial_articles.drop(labels = [\"mil_subj\", \"split_text\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove short articles (< 5 sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "millennial_articles = millennial_articles.loc[millennial_articles[\"num_sentences\"] > 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep for final data restructuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "millennial_articles[\"objects_len\"] = millennial_articles[\"objects\"].apply(len)\n",
    "millennial_articles.loc[millennial_articles[\"objects_len\"] == 0, \"objects\"] = \"\"\n",
    "millennial_articles[\"snippet\"] = millennial_articles[\"text\"].apply(lambda x: \" \".join([i for i in x.split()][0:50]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group verbs by noun chunks and create the finalized JSON objects\n",
    "articles_new = millennial_articles[[\"title_lower\", \"verbs\", \"objects\"]].set_index([\"title_lower\",\"verbs\"])[\"objects\"].apply(pd.Series).stack()\n",
    "articles_new = articles_new.reset_index()\n",
    "\n",
    "articles_new = articles_new.drop(labels = \"level_2\", axis = 1).drop_duplicates()\n",
    "articles_new.columns = [\"title_lower\", \"verbs\", \"objects\"]\n",
    "\n",
    "articles_new[\"article_id\"] = articles_new.index\n",
    "\n",
    "articles_new[\"verbs\"] = articles_new[\"verbs\"].apply(lambda x: x.lower())\n",
    "articles_new[\"objects\"] = articles_new[\"objects\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restructure data for JSON export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build level 1: Verbs and their grouped objects\n",
    "json_level1 = articles_new.loc[(articles_new.objects != \"\") & (articles_new.verbs != \"\")].groupby('verbs')['objects'].apply(set).reset_index()\n",
    "json_level1[\"objects\"] = json_level1[\"objects\"].apply(list)\n",
    "\n",
    "analyzer = SIA()\n",
    "json_level1[\"avg_noun_valence\"] = json_level1[\"objects\"].apply(lambda x: find_valences(x, analyzer) )\n",
    "json_level1.columns = [\"verb\", \"nouns\", \"avg_noun_valence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build level 2: Objects and their grouped verbs/articles\n",
    "json_level2 = articles_new.loc[(articles_new[\"verbs\"] != \"\") & (articles_new[\"objects\"] != \"\")].groupby('objects')['verbs'].apply(set).reset_index()\n",
    "json_level2[\"verbs\"] = json_level2[\"verbs\"].apply(list)\n",
    "\n",
    "grouped_by_articles = articles_new[[\"title_lower\", \"objects\"]].drop_duplicates()\n",
    "grouped_by_articles = grouped_by_articles.groupby('objects')['title_lower'].apply(set).reset_index()\n",
    "grouped_by_articles['title_lower'] = grouped_by_articles['title_lower'].apply(list) \n",
    "\n",
    "json_level2 = pd.merge(left = json_level2, right = grouped_by_articles, how = \"left\", on = \"objects\")\n",
    "json_level2.columns = [\"noun\", \"other_verbs\", \"articles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build level 3: Article metadata\n",
    "json_level3 = millennial_articles[[\"title_lower\", \"url\", \"date\", \"snippet\"]].drop_duplicates()\n",
    "json_level3[\"headline_valence\"] = json_level3[\"title_lower\"].apply(lambda x: analyzer.polarity_scores(x)[\"compound\"])\n",
    "json_level3.columns = [\"headline\", \"url\", \"pub_date\", \"snippet\", \"headline_valence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link level 2 to level 3 through nested dictionaries of headlines corresponding to each noun\n",
    "tmpDataFrame = pd.DataFrame(columns= [\"noun\", \"articles_dict\", \"avg_headline_valence\"])\n",
    "for index, row in json_level2.iterrows():\n",
    "    valences = []\n",
    "    headlines = []\n",
    "    for headline in row[\"articles\"]:\n",
    "        tempdict = json_level3.loc[json_level3[\"headline\"] == headline].to_dict(\"r\")\n",
    "        valences.append(json_level3.loc[json_level3[\"headline\"] == headline, \"headline_valence\"].tolist())\n",
    "        headlines.append(tempdict)\n",
    "    valences = list(chain.from_iterable(valences))\n",
    "    headlines = list(chain.from_iterable(headlines))\n",
    "    tmpDataFrame = tmpDataFrame.append({\"noun\": row[\"noun\"], \"articles_dict\" : tempdict, \"avg_headline_valence\" : pd.np.mean(valences) }, ignore_index = True)\n",
    "del tempdict, headline, valences, headlines, index, row\n",
    "\n",
    "json_level2 = pd.merge(left = json_level2, right = tmpDataFrame, how = \"left\", on = \"noun\").drop(columns = [\"articles\"]).rename(columns = {\"articles_dict\":\"articles\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link level 1 to level 2 through nested dictionaries of nouns corresponding to each verb\n",
    "tmpDataFrame = pd.DataFrame(columns = [\"verb\", \"nouns_dict\"])\n",
    "for index, row in json_level1.iterrows():\n",
    "    holder = []\n",
    "    for noun in row[\"nouns\"]:\n",
    "        tempdf = json_level2.loc[json_level2[\"noun\"] == noun]\n",
    "        tempdf[\"other_verbs\"] = tempdf[\"other_verbs\"].apply(lambda x: [i for i in x if i != row[\"verb\"]])\n",
    "        tempdict = tempdf.to_dict(\"r\")\n",
    "        holder.append(tempdict)\n",
    "    holder = list(chain.from_iterable(holder))\n",
    "    tmpDataFrame = tmpDataFrame.append({\"verb\":row[\"verb\"], \"nouns_dict\":holder}, ignore_index = True)\n",
    "del tempdict, noun, holder, tempdf, index, row\n",
    "\n",
    "json_level1 = pd.merge(left = json_level1, right = tmpDataFrame, how = \"left\", on = \"verb\").drop(columns = [\"nouns\"]).rename(columns = {\"nouns_dict\":\"nouns\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_export = json_level1.to_dict(\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('articles_json.json', 'w') as outfile:\n",
    "    json.dump(for_export, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
